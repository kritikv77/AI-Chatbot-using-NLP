{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('3k_conversations.csv')\n",
        "\n",
        "# Check columns\n",
        "print(data.columns)\n",
        "\n",
        "# Preprocessing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = nltk.word_tokenize(str(text).lower())\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "data['cleaned_question'] = data['question'].apply(preprocess_text)\n",
        "\n",
        "# TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(data['cleaned_question'])\n",
        "\n",
        "# Chatbot response function\n",
        "def chatbot_response(user_input):\n",
        "    user_input_clean = preprocess_text(user_input)\n",
        "    user_vec = vectorizer.transform([user_input_clean])\n",
        "    cosine_similarities = cosine_similarity(user_vec, tfidf_matrix)\n",
        "    most_similar_index = np.argmax(cosine_similarities)\n",
        "    return data['answer'].iloc[most_similar_index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPXoKzZbqi2L",
        "outputId": "e0d7ee3f-ea4d-4e5f-9b22-c27296fb443e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'question', 'answer'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "source": [
        "response = chatbot_response(\"hi how are you\")\n",
        "print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfNcoBGrqywd",
        "outputId": "27525db9-e1d2-4dd8-9dda-2b340750f93d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i'm fine. how about yourself?\n"
          ]
        }
      ]
    },
    {
      "source": [
        "response = chatbot_response(\"what are you standing on?\")\n",
        "print(response)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8nQMaQgrAlC",
        "outputId": "365946b0-4e7b-4201-d088-5cae9ce2bb4b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a couple of dictionaries and some textbooks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ea530f3",
        "outputId": "d8cc81b1-bb13-44d0-b169-a98b1deba87a"
      },
      "source": [
        "# Print a few sample questions from the dataset\n",
        "print(\"Sample questions from the dataset:\")\n",
        "print(data['question'].sample(5).tolist())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample questions from the dataset:\n",
            "['what are you standing on?', 'i think some fish have blue eyes.', 'you hit a white ball.', 'i forgot.', \"they probably wouldn't like that.\"]\n"
          ]
        }
      ]
    }
  ]
}